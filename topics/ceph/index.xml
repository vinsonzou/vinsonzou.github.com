<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ceph on ops</title>
    <link>http://ops.m114.org/topics/ceph/index.xml</link>
    <description>Recent content in Ceph on ops</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2014-2018. All rights reserved.</copyright>
    <atom:link href="http://ops.m114.org/topics/ceph/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ceph核心概念备忘录</title>
      <link>http://ops.m114.org/post/ceph-key-concepts-backup/</link>
      <pubDate>Fri, 26 Aug 2016 14:38:12 +0800</pubDate>
      
      <guid>http://ops.m114.org/post/ceph-key-concepts-backup/</guid>
      <description>

&lt;h2 id=&#34;scrub&#34;&gt;scrub&lt;/h2&gt;

&lt;p&gt;ceph-osd会定义启动scrub线程，扫描部分对象（哪些对象？），和其他副本比较，发现是否一致。如果发现不一致，ceph会抛出这个异常给用户解决。以PG为粒度，触发scrub。用户手动修复，使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph pg repair &amp;lt;pg_id&amp;gt; # 全量复制master节点数据到副本节点。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;scrub分为light scrubbing和Deep scrubbing，前者是频率多直接检查hash值，后者是频率少直接读取内容计算checksum比较。&lt;/p&gt;

&lt;h2 id=&#34;backfill&#34;&gt;backfill&lt;/h2&gt;

&lt;p&gt;当加入或者减少一个新的osd时，所有remapped之后的PG都要迁移到该osd上，此时就叫做backfill。&lt;/p&gt;

&lt;h2 id=&#34;recovery&#34;&gt;recovery&lt;/h2&gt;

&lt;p&gt;当一个osd或者多个osd崩溃之后，再次上线，该osd的状态已经严重滞后了（此时crushmap中还保持该osd）,这个时候就会进行recovery过程。如果是多个osd recovery, 那么这个时候会占用非常多的服务器资源。&lt;/p&gt;

&lt;h2 id=&#34;peering&#34;&gt;peering&lt;/h2&gt;

&lt;p&gt;故障恢复时，对比各个副本的PGlog, 根据PGlog差异构造missing列表，恢复阶段根据missing列表来恢复。peering以PG为单位进行，peering过程中，改PG的IO会被挂起，进入recovery阶段，则可以接受IO，但hit到missing列表项的，也会挂起，直到恢复完成后。因为PGlog的记录是有限的，当peering时发现，PGlog差异太大，则会触发backfill。&lt;/p&gt;

&lt;h2 id=&#34;active-clean&#34;&gt;active + clean&lt;/h2&gt;

&lt;p&gt;PG的status，active的意思是说该PG可以接受读写请求，clean的意思是说PG的副本数达到了要求。&lt;/p&gt;

&lt;h2 id=&#34;degrade&#34;&gt;degrade&lt;/h2&gt;

&lt;p&gt;PG的副本数没有达到要求，但是满足最小副本数要求。&lt;/p&gt;

&lt;h2 id=&#34;incomplete&#34;&gt;incomplete&lt;/h2&gt;

&lt;p&gt;PG的副本数连最小副本数都没有达到。&lt;/p&gt;

&lt;h2 id=&#34;inconsistent&#34;&gt;inconsistent&lt;/h2&gt;

&lt;p&gt;scrub或者deep scrub的时候发现PG内容不一致。&lt;/p&gt;

&lt;h2 id=&#34;down&#34;&gt;down&lt;/h2&gt;

&lt;p&gt;关键数据丢失。进入这个状态的一种方法，比如一个PG有两个副本，先down掉其中的一个osd，再down掉第二个osd，最后把第一个osd起起来，这样这个PG就处于down状态。&lt;/p&gt;

&lt;h2 id=&#34;pglog和日志文件系统&#34;&gt;PGlog和日志文件系统&lt;/h2&gt;

&lt;p&gt;PGlog相当于undo log, journal相当于redo log。一个是在某个操作执行完成之后，做log记录，如果操作成功，则可以undo；另一个是在某个操作执行之前，做log记录，如果操作失败，下次可以redo。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ceph pgs inconsistent</title>
      <link>http://ops.m114.org/post/ceph-pgs-inconsistent/</link>
      <pubDate>Wed, 17 Aug 2016 23:44:59 +0800</pubDate>
      
      <guid>http://ops.m114.org/post/ceph-pgs-inconsistent/</guid>
      <description>&lt;p&gt;OSD扩容后出现如下错误&lt;/p&gt;

&lt;p&gt;&lt;code&gt;HEALTH_ERR 2 pgs inconsistent; 320 scrub errors&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ceph01 ~]# ceph health detail
HEALTH_ERR 2 pgs inconsistent; 320 scrub errors
pg 10.9 is active+clean+inconsistent, acting [6,0,9]
pg 10.10 is active+clean+scrubbing+deep+inconsistent, acting [2,6,4]
320 scrub errors
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修复处于不一致状态的pgs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ceph01 ~]# ceph pg repair 10.9
instructing pg 10.9 on osd.6 to repair

[root@ceph01 ~]# ceph pg repair 10.10
instructing pg 10.10 on osd.2 to repair
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过一段时间的修复后，ceph恢复正常。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>删除osd的正确方式</title>
      <link>http://ops.m114.org/post/right-steps-delete-ceph-osd/</link>
      <pubDate>Sat, 06 Aug 2016 23:51:26 +0800</pubDate>
      
      <guid>http://ops.m114.org/post/right-steps-delete-ceph-osd/</guid>
      <description>

&lt;p&gt;按照官网的步骤走的话，在 &lt;code&gt;标记osd为out&lt;/code&gt; 和 &lt;code&gt;从crushmap删除osd&lt;/code&gt; 这两步都会触发数据再平衡，如下方式只触发了一次迁移，建议使用。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;h2 id=&#34;调整osd的crush-weight&#34;&gt;调整osd的crush weight&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd crush reweight osd.0 0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：这个地方如果想慢慢的调整就分几次将crush 的weight 减低到0 ，这个过程实际上是让数据不分布在这个节点上，让数据慢慢的分布到其他节点上，直到最终为没有分布在这个osd，并且迁移完成这个地方不光调整了osd 的crush weight ，实际上同时调整了host 的 weight ，这样会调整集群的整体的crush 分布，在osd 的crush 为0 后， 再对这个osd的任何删除相关操作都不会影响到集群数据的分布。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;停止osd进程&#34;&gt;停止osd进程&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;systemctl stop ceph-osd@0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是通知集群这个osd进程不在了，不提供服务了，因为本身没权重，就不会影响到整体的分布，也就没有迁移&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;将节点状态标记为out&#34;&gt;将节点状态标记为out&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd out osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是通知集群这个osd不再映射数据了，不提供服务了，因为本身没权重，就不会影响到整体的分布，也就没有迁移&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;从crush中移除节点&#34;&gt;从crush中移除节点&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd crush remove osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是从crush中删除，因为已经是0了 所以没影响主机的权重，也就没有迁移了&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;删除节点&#34;&gt;删除节点&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd rm osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是从集群里面删除这个节点的记录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;删除节点认证-不删除编号会占住&#34;&gt;删除节点认证（不删除编号会占住)&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph auth del osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是从认证当中删除这个节点的信息&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>ceph集群jewel版本部署osd激活权限报错</title>
      <link>http://ops.m114.org/post/ceph-jewel-osd-activate-bug/</link>
      <pubDate>Fri, 29 Jul 2016 00:29:29 +0800</pubDate>
      
      <guid>http://ops.m114.org/post/ceph-jewel-osd-activate-bug/</guid>
      <description>&lt;p&gt;&lt;strong&gt;环境&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)&lt;/br&gt;
ceph-deploy 1.5.34&lt;/p&gt;

&lt;p&gt;ceph集群jewel版本部署过程中执行osd激活操作如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate ceph13:/dev/sdb1:/dev/sda2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错内容如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2016-07-29 00:05:19,106][ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[2016-07-29 00:05:19,107][ceph_deploy.cli][INFO  ] Invoked (1.5.34): /bin/ceph-deploy osd activate ceph13:/dev/sdb1:/dev/sda2
[2016-07-29 00:05:19,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  username                      : None
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  verbose                       : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  quiet                         : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  cd_conf                       : &amp;lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x26dcb90&amp;gt;
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  func                          : &amp;lt;function osd at 0x26d0320&amp;gt;
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  default_release               : False
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  disk                          : [(&#39;ceph13&#39;, &#39;/dev/sdb1&#39;, &#39;/dev/sda2&#39;)]
[2016-07-29 00:05:19,110][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks ceph13:/dev/sdb1:/dev/sda2
[2016-07-29 00:05:19,217][ceph13][DEBUG ] connected to host: ceph13
[2016-07-29 00:05:19,218][ceph13][DEBUG ] detect platform information from remote host
[2016-07-29 00:05:19,258][ceph13][DEBUG ] detect machine type
[2016-07-29 00:05:19,264][ceph13][DEBUG ] find the location of an executable
[2016-07-29 00:05:19,266][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.2.1511 Core
[2016-07-29 00:05:19,266][ceph_deploy.osd][DEBUG ] activating host ceph13 disk /dev/sdb1
[2016-07-29 00:05:19,266][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2016-07-29 00:05:19,266][ceph13][DEBUG ] find the location of an executable
[2016-07-29 00:05:19,270][ceph13][INFO  ] Running command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2016-07-29 00:05:19,598][ceph13][WARNING] main_activate: path = /dev/sdb1
[2016-07-29 00:05:19,598][ceph13][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2016-07-29 00:05:19,598][ceph13][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2016-07-29 00:05:19,630][ceph13][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2016-07-29 00:05:19,638][ceph13][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2016-07-29 00:05:19,803][ceph13][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2016-07-29 00:05:19,967][ceph13][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.K5WBsO with options noatime,inode64
[2016-07-29 00:05:19,967][ceph13][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:19,983][ceph13][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:19,991][ceph13][WARNING] activate: Cluster uuid is f2694afb-b5fc-4225-982a-342c4a1ca389
[2016-07-29 00:05:19,991][ceph13][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: Cluster name is ceph
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: OSD uuid is 684effdc-67ae-4f54-a9b8-a113f4a8f0cc
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: OSD id is 0
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: Initializing OSD...
[2016-07-29 00:05:20,157][ceph13][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap
[2016-07-29 00:05:20,572][ceph13][WARNING] got monmap epoch 1
[2016-07-29 00:05:20,572][ceph13][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap --osd-data /var/lib/ceph/tmp/mnt.K5WBsO --osd-journal /var/lib/ceph/tmp/mnt.K5WBsO/journal --osd-uuid 684effdc-67ae-4f54-a9b8-a113f4a8f0cc --keyring /var/lib/ceph/tmp/mnt.K5WBsO/keyring --setuser ceph --setgroup ceph
[2016-07-29 00:05:20,686][ceph13][WARNING] mount_activate: Failed to activate
[2016-07-29 00:05:20,686][ceph13][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:20,687][ceph13][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:21,052][ceph13][WARNING] Traceback (most recent call last):
[2016-07-29 00:05:21,052][ceph13][WARNING]   File &amp;quot;/usr/sbin/ceph-disk&amp;quot;, line 9, in &amp;lt;module&amp;gt;
[2016-07-29 00:05:21,052][ceph13][WARNING]     load_entry_point(&#39;ceph-disk==1.0.0&#39;, &#39;console_scripts&#39;, &#39;ceph-disk&#39;)()
[2016-07-29 00:05:21,052][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 4994, in run
[2016-07-29 00:05:21,053][ceph13][WARNING]     main(sys.argv[1:])
[2016-07-29 00:05:21,053][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 4945, in main
[2016-07-29 00:05:21,053][ceph13][WARNING]     args.func(args)
[2016-07-29 00:05:21,053][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3299, in main_activate
[2016-07-29 00:05:21,053][ceph13][WARNING]     reactivate=args.reactivate,
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3056, in mount_activate
[2016-07-29 00:05:21,054][ceph13][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3232, in activate
[2016-07-29 00:05:21,054][ceph13][WARNING]     keyring=keyring,
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 2725, in mkfs
[2016-07-29 00:05:21,055][ceph13][WARNING]     &#39;--setgroup&#39;, get_ceph_group(),
[2016-07-29 00:05:21,055][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 2672, in ceph_osd_mkfs
[2016-07-29 00:05:21,055][ceph13][WARNING]     raise Error(&#39;%s failed : %s&#39; % (str(arguments), error))
[2016-07-29 00:05:21,055][ceph13][WARNING] ceph_disk.main.Error: Error: [&#39;ceph-osd&#39;, &#39;--cluster&#39;, &#39;ceph&#39;, &#39;--mkfs&#39;, &#39;--mkkey&#39;, &#39;-i&#39;, &#39;0&#39;, &#39;--monmap&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap&#39;, &#39;--osd-data&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO&#39;, &#39;--osd-journal&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/journal&#39;, &#39;--osd-uuid&#39;, &#39;684effdc-67ae-4f54-a9b8-a113f4a8f0cc&#39;, &#39;--keyring&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/keyring&#39;, &#39;--setuser&#39;, &#39;ceph&#39;, &#39;--setgroup&#39;, &#39;ceph&#39;] failed : 2016-07-29 00:05:20.672440 7f46517dd800 -1 filestore(/var/lib/ceph/tmp/mnt.K5WBsO) mkjournal error creating journal on /var/lib/ceph/tmp/mnt.K5WBsO/journal: (13) Permission denied
[2016-07-29 00:05:21,056][ceph13][WARNING] 2016-07-29 00:05:20.672462 7f46517dd800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -13
[2016-07-29 00:05:21,056][ceph13][WARNING] 2016-07-29 00:05:20.672526 7f46517dd800 -1  ** ERROR: error creating empty object store in /var/lib/ceph/tmp/mnt.K5WBsO: (13) Permission denied
[2016-07-29 00:05:21,056][ceph13][WARNING]
[2016-07-29 00:05:21,057][ceph13][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2016-07-29 00:05:21,057][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;解决办法:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将ceph集群需要使用的所有磁盘权限，所属用户、用户组改给ceph&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chown ceph:ceph /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;问题延伸:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;此问题本次修复后，系统重启磁盘权限会被修改回，导致osd服务无法正常启动，这个权限问题很坑，每次系统启动自动修改磁盘权限。&lt;/p&gt;

&lt;p&gt;查找ceph资料，发现这其实是一个bug，社区暂未解决。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tracker.ceph.com/issues/13833&#34;&gt;http://tracker.ceph.com/issues/13833&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用国内源部署Ceph</title>
      <link>http://ops.m114.org/post/deploy-ceph-using-china-mirror/</link>
      <pubDate>Thu, 28 Jul 2016 13:17:52 +0800</pubDate>
      
      <guid>http://ops.m114.org/post/deploy-ceph-using-china-mirror/</guid>
      <description>

&lt;p&gt;由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。&lt;/p&gt;

&lt;h1 id=&#34;关于国内源&#34;&gt;关于国内源&lt;/h1&gt;

&lt;p&gt;根据Ceph中国社区的统计，国内已经有四家网站开始同步Ceph源，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;网易镜像源&lt;a href=&#34;http://mirrors.163.com/ceph&#34;&gt;http://mirrors.163.com/ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;阿里镜像源&lt;a href=&#34;http://mirrors.aliyun.com/ceph&#34;&gt;http://mirrors.aliyun.com/ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中科大镜像源&lt;a href=&#34;http://mirrors.ustc.edu.cn/ceph&#34;&gt;http://mirrors.ustc.edu.cn/ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;宝德镜像源&lt;a href=&#34;http://mirrors.plcloud.com/ceph&#34;&gt;http://mirrors.plcloud.com/ceph&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装ceph-jewel版&#34;&gt;安装Ceph(Jewel版)&lt;/h1&gt;

&lt;p&gt;由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export CEPH_DEPLOY_REPO_URL=http://mirrors.aliyun.com/ceph/rpm-jewel/el7
export CEPH_DEPLOY_GPG_URL=http://mirrors.aliyun.com/ceph/keys/release.asc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后的过程就没任何区别了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Create monitor node
ceph-deploy new node1 node2 node3

# Software Installation
ceph-deploy install deploy node1 node2 node3

# Gather keys
ceph-deploy mon create-initial

# Ceph deploy osd create disk
ceph-deploy osd create node1:sdb:/dev/sdc
ceph-deploy osd create node2:sdb:/dev/sdc
ceph-deploy osd create node3:sdb:/dev/sdc

# Make 3 copies by default
echo &amp;quot;osd pool default size = 3&amp;quot; | tee -a $HOME/ceph.conf

# Copy admin keys and configuration files
ceph-deploy --overwrite-conf admin deploy node1 node2 node3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以很快速的使用国内源创建出Ceph集群，希望能对大家日常的使用提供便捷。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>