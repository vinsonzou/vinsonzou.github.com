<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ops</title>
    <link>https://ops.m114.org/tags/ceph/index.xml</link>
    <description>Recent content on ops</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2014-2016. All rights reserved.</copyright>
    <atom:link href="https://ops.m114.org/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ceph集群jewel版本部署osd激活权限报错</title>
      <link>https://ops.m114.org/post/ceph-jewel-osd-activate-bug/</link>
      <pubDate>Fri, 29 Jul 2016 00:29:29 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/ceph-jewel-osd-activate-bug/</guid>
      <description>&lt;p&gt;&lt;strong&gt;环境&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)&lt;/br&gt;
ceph-deploy 1.5.34&lt;/p&gt;

&lt;p&gt;ceph集群jewel版本部署过程中执行osd激活操作如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate ceph13:/dev/sdb1:/dev/sda2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错内容如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2016-07-29 00:05:19,106][ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[2016-07-29 00:05:19,107][ceph_deploy.cli][INFO  ] Invoked (1.5.34): /bin/ceph-deploy osd activate ceph13:/dev/sdb1:/dev/sda2
[2016-07-29 00:05:19,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  username                      : None
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  verbose                       : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  quiet                         : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  cd_conf                       : &amp;lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x26dcb90&amp;gt;
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  func                          : &amp;lt;function osd at 0x26d0320&amp;gt;
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  default_release               : False
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  disk                          : [(&#39;ceph13&#39;, &#39;/dev/sdb1&#39;, &#39;/dev/sda2&#39;)]
[2016-07-29 00:05:19,110][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks ceph13:/dev/sdb1:/dev/sda2
[2016-07-29 00:05:19,217][ceph13][DEBUG ] connected to host: ceph13
[2016-07-29 00:05:19,218][ceph13][DEBUG ] detect platform information from remote host
[2016-07-29 00:05:19,258][ceph13][DEBUG ] detect machine type
[2016-07-29 00:05:19,264][ceph13][DEBUG ] find the location of an executable
[2016-07-29 00:05:19,266][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.2.1511 Core
[2016-07-29 00:05:19,266][ceph_deploy.osd][DEBUG ] activating host ceph13 disk /dev/sdb1
[2016-07-29 00:05:19,266][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2016-07-29 00:05:19,266][ceph13][DEBUG ] find the location of an executable
[2016-07-29 00:05:19,270][ceph13][INFO  ] Running command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2016-07-29 00:05:19,598][ceph13][WARNING] main_activate: path = /dev/sdb1
[2016-07-29 00:05:19,598][ceph13][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2016-07-29 00:05:19,598][ceph13][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2016-07-29 00:05:19,630][ceph13][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2016-07-29 00:05:19,638][ceph13][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2016-07-29 00:05:19,803][ceph13][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2016-07-29 00:05:19,967][ceph13][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.K5WBsO with options noatime,inode64
[2016-07-29 00:05:19,967][ceph13][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:19,983][ceph13][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:19,991][ceph13][WARNING] activate: Cluster uuid is f2694afb-b5fc-4225-982a-342c4a1ca389
[2016-07-29 00:05:19,991][ceph13][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: Cluster name is ceph
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: OSD uuid is 684effdc-67ae-4f54-a9b8-a113f4a8f0cc
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: OSD id is 0
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: Initializing OSD...
[2016-07-29 00:05:20,157][ceph13][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap
[2016-07-29 00:05:20,572][ceph13][WARNING] got monmap epoch 1
[2016-07-29 00:05:20,572][ceph13][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap --osd-data /var/lib/ceph/tmp/mnt.K5WBsO --osd-journal /var/lib/ceph/tmp/mnt.K5WBsO/journal --osd-uuid 684effdc-67ae-4f54-a9b8-a113f4a8f0cc --keyring /var/lib/ceph/tmp/mnt.K5WBsO/keyring --setuser ceph --setgroup ceph
[2016-07-29 00:05:20,686][ceph13][WARNING] mount_activate: Failed to activate
[2016-07-29 00:05:20,686][ceph13][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:20,687][ceph13][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:21,052][ceph13][WARNING] Traceback (most recent call last):
[2016-07-29 00:05:21,052][ceph13][WARNING]   File &amp;quot;/usr/sbin/ceph-disk&amp;quot;, line 9, in &amp;lt;module&amp;gt;
[2016-07-29 00:05:21,052][ceph13][WARNING]     load_entry_point(&#39;ceph-disk==1.0.0&#39;, &#39;console_scripts&#39;, &#39;ceph-disk&#39;)()
[2016-07-29 00:05:21,052][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 4994, in run
[2016-07-29 00:05:21,053][ceph13][WARNING]     main(sys.argv[1:])
[2016-07-29 00:05:21,053][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 4945, in main
[2016-07-29 00:05:21,053][ceph13][WARNING]     args.func(args)
[2016-07-29 00:05:21,053][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3299, in main_activate
[2016-07-29 00:05:21,053][ceph13][WARNING]     reactivate=args.reactivate,
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3056, in mount_activate
[2016-07-29 00:05:21,054][ceph13][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3232, in activate
[2016-07-29 00:05:21,054][ceph13][WARNING]     keyring=keyring,
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 2725, in mkfs
[2016-07-29 00:05:21,055][ceph13][WARNING]     &#39;--setgroup&#39;, get_ceph_group(),
[2016-07-29 00:05:21,055][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 2672, in ceph_osd_mkfs
[2016-07-29 00:05:21,055][ceph13][WARNING]     raise Error(&#39;%s failed : %s&#39; % (str(arguments), error))
[2016-07-29 00:05:21,055][ceph13][WARNING] ceph_disk.main.Error: Error: [&#39;ceph-osd&#39;, &#39;--cluster&#39;, &#39;ceph&#39;, &#39;--mkfs&#39;, &#39;--mkkey&#39;, &#39;-i&#39;, &#39;0&#39;, &#39;--monmap&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap&#39;, &#39;--osd-data&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO&#39;, &#39;--osd-journal&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/journal&#39;, &#39;--osd-uuid&#39;, &#39;684effdc-67ae-4f54-a9b8-a113f4a8f0cc&#39;, &#39;--keyring&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/keyring&#39;, &#39;--setuser&#39;, &#39;ceph&#39;, &#39;--setgroup&#39;, &#39;ceph&#39;] failed : 2016-07-29 00:05:20.672440 7f46517dd800 -1 filestore(/var/lib/ceph/tmp/mnt.K5WBsO) mkjournal error creating journal on /var/lib/ceph/tmp/mnt.K5WBsO/journal: (13) Permission denied
[2016-07-29 00:05:21,056][ceph13][WARNING] 2016-07-29 00:05:20.672462 7f46517dd800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -13
[2016-07-29 00:05:21,056][ceph13][WARNING] 2016-07-29 00:05:20.672526 7f46517dd800 -1  ** ERROR: error creating empty object store in /var/lib/ceph/tmp/mnt.K5WBsO: (13) Permission denied
[2016-07-29 00:05:21,056][ceph13][WARNING]
[2016-07-29 00:05:21,057][ceph13][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2016-07-29 00:05:21,057][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;解决办法:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将ceph集群需要使用的所有磁盘权限，所属用户、用户组改给ceph&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chown ceph:ceph /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;问题延伸:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;此问题本次修复后，系统重启磁盘权限会被修改回，导致osd服务无法正常启动，这个权限问题很坑，每次系统启动自动修改磁盘权限。&lt;/p&gt;

&lt;p&gt;查找ceph资料，发现这其实是一个bug，社区暂未解决。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tracker.ceph.com/issues/13833&#34;&gt;http://tracker.ceph.com/issues/13833&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用国内源部署Ceph</title>
      <link>https://ops.m114.org/post/deploy-ceph-using-china-mirror/</link>
      <pubDate>Thu, 28 Jul 2016 13:17:52 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/deploy-ceph-using-china-mirror/</guid>
      <description>

&lt;p&gt;由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。&lt;/p&gt;

&lt;h1 id=&#34;关于国内源&#34;&gt;关于国内源&lt;/h1&gt;

&lt;p&gt;根据Ceph中国社区的统计，国内已经有四家网站开始同步Ceph源，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;网易镜像源&lt;a href=&#34;http://mirrors.163.com/ceph&#34;&gt;http://mirrors.163.com/ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;阿里镜像源&lt;a href=&#34;http://mirrors.aliyun.com/ceph&#34;&gt;http://mirrors.aliyun.com/ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中科大镜像源&lt;a href=&#34;http://mirrors.ustc.edu.cn/ceph&#34;&gt;http://mirrors.ustc.edu.cn/ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;宝德镜像源&lt;a href=&#34;http://mirrors.plcloud.com/ceph&#34;&gt;http://mirrors.plcloud.com/ceph&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装ceph-jewel版&#34;&gt;安装Ceph(Jewel版)&lt;/h1&gt;

&lt;p&gt;由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export CEPH_DEPLOY_REPO_URL=http://mirrors.aliyun.com/ceph/rpm-jewel/el7
export CEPH_DEPLOY_GPG_URL=http://mirrors.aliyun.com/ceph/keys/release.asc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后的过程就没任何区别了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Create monitor node
ceph-deploy new node1 node2 node3

# Software Installation
ceph-deploy install deploy node1 node2 node3

# Gather keys
ceph-deploy mon create-initial

# Ceph deploy osd create disk
ceph-deploy osd create node1:sdb:/dev/sdc
ceph-deploy osd create node2:sdb:/dev/sdc
ceph-deploy osd create node3:sdb:/dev/sdc

# Make 3 copies by default
echo &amp;quot;osd pool default size = 3&amp;quot; | tee -a $HOME/ceph.conf

# Copy admin keys and configuration files
ceph-deploy --overwrite-conf admin deploy node1 node2 node3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以很快速的使用国内源创建出Ceph集群，希望能对大家日常的使用提供便捷。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>