<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ops</title>
    <link>https://ops.m114.org/index.xml</link>
    <description>Recent content on ops</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2014-2017. All rights reserved.</copyright>
    <lastBuildDate>Sun, 23 Jul 2017 16:32:26 +0800</lastBuildDate>
    <atom:link href="https://ops.m114.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>analysing java core dump</title>
      <link>https://ops.m114.org/post/analysing-java-core-dump/</link>
      <pubDate>Sun, 23 Jul 2017 16:32:26 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/analysing-java-core-dump/</guid>
      <description>&lt;p&gt;In this post, I will show you how you can debug a Java core file to see what caused your JVM to crash. I will be using a core file I generated in my previous post: &lt;a href=&#34;http://fahdshariff.blogspot.co.uk/2012/08/generating-java-core-dump.html&#34;&gt;Generating a Java Core Dump&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are different ways you can diagnose a JVM crash, listed below:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The hs_err_pid log file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When a fatal error occurs in the JVM, it produces an error log file called hs_err_pidXXXX.log, normally in the working directory of the process or in the temporary directory for the operating system. The top of this file contains the cause of the crash and the &amp;ldquo;problematic frame&amp;rdquo;. For example, mine shows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ head hs_err_pid21178.log
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x0000002b1d00075c, pid=21178, tid=1076017504
#
# JRE version: 6.0_21-b06
# Java VM: Java HotSpot(TM) 64-Bit Server VM (17.0-b16 mixed mode linux-amd64 )
# Problematic frame:
# C  [libnativelib.so+0x75c]  bar+0x10
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is also a stack trace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Stack: [0x000000004012b000,0x000000004022c000],  sp=0x000000004022aac0,  free space=3fe0000000000000018k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libnativelib.so+0x75c]  bar+0x10
C  [libnativelib.so+0x772]  foo+0xe
C  [libnativelib.so+0x78e]  Java_CoreDumper_core+0x1a
j  CoreDumper.core()V+0
j  CoreDumper.main([Ljava/lang/String;)V+7
v  ~StubRoutines::call_stub
V  [libjvm.so+0x3e756d]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The stack trace shows that my java method, CoreDumper.core(), called into JNI and died when the bar function was called in native code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Debugging a Java Core Dump&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In some cases, the JVM may not produce a hs_err_pid file, for example, if the native code abruptly aborts by calling the abort function. In such cases, we need to analyse the core file produced. On my machine, the operating system writes out core files to /var/tmp/cores. You can use the following command to see where your system is configured to write out core files to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /proc/sys/kernel/core_pattern
/var/tmp/cores/%e.%p.%u.core
$ ls /var/tmp/cores
java.21178.146385.core
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few, different ways to look at core dumps:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Using gdb&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GNU Debugger (gdb) can examine a core file and work out what the program was doing when it crashed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gdb $JAVA_HOME/bin/java /var/tmp/cores/java.14015.146385.core
(gdb) where
#0  0x0000002a959bd26d in raise () from /lib64/tls/libc.so.6
#1  0x0000002a959bea6e in abort () from /lib64/tls/libc.so.6
#2  0x0000002b1cecf799 in bar () from libnativelib.so
#3  0x0000002b1cecf7a7 in foo () from libnativelib.so
#4  0x0000002b1cecf7c3 in Java_CoreDumper_core () from libnativelib.so
#5  0x0000002a971aac88 in ?? ()
#6  0x0000000040113800 in ?? ()
#7  0x0000002a9719fa42 in ?? ()
#8  0x000000004022ab10 in ?? ()
#9  0x0000002a9a4d5488 in ?? ()
#10 0x000000004022ab70 in ?? ()
#11 0x0000002a9a4d59c8 in ?? ()
#12 0x0000000000000000 in ?? ()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The where command prints the stack frames and shows that the bar function called abort() which caused the crash.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Using jstack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;jstack prints stack traces of Java threads for a given core file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ jstack -J-d64 $JAVA_HOME/bin/java /var/tmp/cores/java.14015.146385.core
Debugger attached successfully.
Server compiler detected.
JVM version is 17.0-b16
Deadlock Detection:

No deadlocks found.

Thread 16788: (state = BLOCKED)

Thread 16787: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.ref.ReferenceQueue.remove(long) @bci=44, line=118 (Interpreted frame)
 - java.lang.ref.ReferenceQueue.remove() @bci=2, line=134 (Interpreted frame)
 - java.lang.ref.Finalizer$FinalizerThread.run() @bci=3, line=159 (Interpreted frame)

Thread 16786: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Object.wait() @bci=2, line=485 (Interpreted frame)
 - java.lang.ref.Reference$ReferenceHandler.run() @bci=46, line=116 (Interpreted frame)

Thread 16780: (state = IN_NATIVE)
 - CoreDumper.core() @bci=0 (Interpreted frame)
 - CoreDumper.main(java.lang.String[]) @bci=7, line=12 (Interpreted frame)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3. Using jmap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;jmap examines a core file and prints out shared object memory maps or heap memory details.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ jmap -J-d64 $JAVA_HOME/bin/java /var/tmp/cores/java.14015.146385.core
Debugger attached successfully.
Server compiler detected.
JVM version is 17.0-b16
0x0000000040000000      49K     /usr/sunjdk/1.6.0_21/bin/java
0x0000002a9566c000      124K    /lib64/tls/libpthread.so.0
0x0000002a95782000      47K     /usr/sunjdk/1.6.0_21/jre/lib/amd64/jli/libjli.so
0x0000002a9588c000      16K     /lib64/libdl.so.2
0x0000002a9598f000      1593K   /lib64/tls/libc.so.6
0x0000002a95556000      110K    /lib64/ld-linux-x86-64.so.2
0x0000002a95bca000      11443K  /usr/sunjdk/1.6.0_21/jre/lib/amd64/server/libjvm.so
0x0000002a96699000      625K    /lib64/tls/libm.so.6
0x0000002a9681f000      56K     /lib64/tls/librt.so.1
0x0000002a96939000      65K     /usr/sunjdk/1.6.0_21/jre/lib/amd64/libverify.so
0x0000002a96a48000      228K    /usr/sunjdk/1.6.0_21/jre/lib/amd64/libjava.so
0x0000002a96b9e000      109K    /lib64/libnsl.so.1
0x0000002a96cb6000      54K     /usr/sunjdk/1.6.0_21/jre/lib/amd64/native_threads/libhpi.so
0x0000002a96de8000      57K     /lib64/libnss_files.so.2
0x0000002a96ef4000      551K    /lib64/libnss_db.so.2
0x0000002a97086000      89K     /usr/sunjdk/1.6.0_21/jre/lib/amd64/libzip.so
0x0000002b1cecf000      6K      /home/sharfah/tmp/jni/libnativelib.so
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Useful Links:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://weblogs.java.net/blog/kohsuke/archive/2009/02/crash_course_on.html&#34;&gt;Crash course on JVM crash analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fahdshariff.blogspot.co.uk/2012/08/generating-java-core-dump.html&#34;&gt;Generating a Java Core Dump&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From &lt;a href=&#34;http://fahdshariff.blogspot.co.uk/2012/08/analysing-java-core-dump.html&#34;&gt;http://fahdshariff.blogspot.co.uk/2012/08/analysing-java-core-dump.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Error: 500 OOPS: priv_sock_get_cmd [SOLVED]</title>
      <link>https://ops.m114.org/post/error-500-oops-priv_sock_get_cmd-solved/</link>
      <pubDate>Sun, 23 Jul 2017 16:21:45 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/error-500-oops-priv_sock_get_cmd-solved/</guid>
      <description>

&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;seccomp filter sanboxing with vsftpd 3.0.x&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following error may occur on ftp clients with vsftpd 3.0.x:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;500 OOPS: priv_sock_get_cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is caused by &lt;a href=&#34;http://en.wikipedia.org/wiki/Seccomp&#34;&gt;seccomp filter sanboxing&lt;/a&gt;, which is enabled by default on &lt;code&gt;amd64&lt;/code&gt;. To workaround this issue, disable seccomp filter sanboxing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;seccomp_sandbox=NO&#39; &amp;gt;&amp;gt; vsftpd.conf
service vsftpd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For further information, refer to &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=845980&#34;&gt;Red Hat bug #845980&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python版本号比较</title>
      <link>https://ops.m114.org/post/python-version-cmp/</link>
      <pubDate>Sun, 23 Jul 2017 16:13:10 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/python-version-cmp/</guid>
      <description>

&lt;h2 id=&#34;第一种比较方法-strictversion&#34;&gt;第一种比较方法(StrictVersion)&lt;/h2&gt;

&lt;p&gt;StrictVersion是由.将一串带有预发布标签的数字分隔为两个或三个部分的格式，预发布标签的字母只能是a或者b加数字版本号，而且只能在最末尾。预发布a版本低于b版本，并且预发布版本永远小于正式发布版本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;合法格式:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.4       0.4.0  (相同版本)
0.4.1
0.5a1     (预发布版本a1，小于0.5，即0.5版本更新)
0.5b3
0.5
0.9.6
1.0
1.0.4a3
1.0.4b1
1.0.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;非法格式:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1           没有.分隔，需要分隔为2-3部分
2.7.2.2     被分隔成了4个部分
1.3.a4      预发布版本号应该在数字后面
1.3pl1      预发布版本号字母标签只能是a或者b
1.3B1       预发布版本号字母标签只能是a或者b
1.3c        预发布版本号字母标签后必须加数字版本号
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;版本比较&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: from distutils.version import StrictVersion

In [2]: StrictVersion(&#39;1.2a3&#39;) &amp;lt; StrictVersion(&#39;1.2b1&#39;)
Out[2]: True

In [3]: StrictVersion(&#39;1.2b1&#39;) &amp;lt; StrictVersion(&#39;1.2&#39;)
Out[3]: True

In [4]: StrictVersion(&#39;1.2&#39;) &amp;lt; StrictVersion(&#39;1.2.1&#39;)
Out[4]: True

In [5]: StrictVersion(&#39;1.2&#39;) == StrictVersion(&#39;1.2.0&#39;)
Out[5]: True

In [6]: StrictVersion(&#39;1.2.11&#39;) &amp;lt; StrictVersion(&#39;1.11&#39;)
Out[6]: True
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;第二种比较方法-looseversion&#34;&gt;第二种比较方法(LooseVersion)&lt;/h2&gt;

&lt;p&gt;LooseVersion格式要求和StrictVersion不同，或者说它并没有任何规定的格式。由一系列数字,相隔时间或字母的字符串组成，并没有一个严格的格式。在进行比较的时候按照数字大小，字符串按字典顺序比较。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;合法格式&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.5.1
1.5.2b2
161
3.10a
8.02
3.4j
1996.07.12
3.2.pl0
3.1.1.6
2g6
11g
0.960923
2.2beta29
1.13++
5.5.kw
2.0b1pl0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;非法格式&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;并没有哟
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;格式比较&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: from distutils.version import StrictVersion

In [2]: LooseVersion(&#39;1.6.0x&#39;) &amp;lt; LooseVersion(&#39;1.20.0x&#39;)
Out[2]: True

In [3]: LooseVersion(&#39;2.6.0x&#39;) &amp;lt; LooseVersion(&#39;1.20.0x&#39;)
Out[3]: False

In [4]: LooseVersion(&#39;1.20.0x&#39;) &amp;lt; LooseVersion(&#39;1.20.0z&#39;)
Out[4]: True

In [5]: LooseVersion(&#39;1&#39;) &amp;lt; LooseVersion(&#39;a&#39;)
Out[5]: True 
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>golang sync.WaitGroup解决goroutine同步</title>
      <link>https://ops.m114.org/post/sync-WaitGroup/</link>
      <pubDate>Mon, 03 Jul 2017 16:27:12 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/sync-WaitGroup/</guid>
      <description>&lt;p&gt;&lt;strong&gt;go提供了sync包和channel来解决协程同步和通讯。&lt;/strong&gt;新手对channel通道操作起来更容易产生死锁，如果时缓冲的channel还要考虑channel放入和取出数据的速率问题。&lt;/p&gt;

&lt;p&gt;从字面就可以理解，sync.WaitGroup是等待一组协程结束。它实现了一个类似任务队列的结构，你可以向队列中加入任务，任务完成后就把任务从队列中移除，如果队列中的任务没有全部完成，队列就会触发阻塞以阻止程序继续运行。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;sync.WaitGroup只有3个方法，Add()，Done()，Wait()。&lt;/strong&gt; 其中Done()是Add(-1)的别名。简单的来说，使用Add()添加计数，Done()减掉一个计数，计数不为0, 阻塞Wait()的运行。&lt;/p&gt;

&lt;p&gt;简单示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
 
import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;sync&amp;quot;
)
 
var waitgroup sync.WaitGroup
 
func test(shownum int) {
	fmt.Println(shownum)
	waitgroup.Done() //任务完成，将任务队列中的任务数量-1，其实.Done就是.Add(-1)
}
 
func main() {
	for i := 0; i &amp;lt; 10; i++ {
		waitgroup.Add(1) //每创建一个goroutine，就把任务队列中任务的数量+1
		go test(i)
	}
	waitgroup.Wait() //.Wait()这里会发生阻塞，直到队列中所有的任务结束就会解除阻塞
	fmt.Println(&amp;quot;done!&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ps: 此文为学习记录，如有错误还请多指教。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tomcat 7.0.76 Invalid character found in the request target</title>
      <link>https://ops.m114.org/post/fix-tomcat-upgrade-error/</link>
      <pubDate>Mon, 27 Mar 2017 13:08:25 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/fix-tomcat-upgrade-error/</guid>
      <description>&lt;p&gt;&lt;strong&gt;故障现象&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;升级tomcat至7.0.76后，GET请求的参数中含有中文时tomcat返回400错误，tomcat错误日志如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java.lang.IllegalArgumentException: Invalid character found in the request target. The valid characters are defined in RFC 7230 and RFC 3986
	at org.apache.coyote.http11.InternalNioInputBuffer.parseRequestLine(InternalNioInputBuffer.java:317)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1000)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1756)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1715)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:662)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;原因&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;查询&lt;a href=&#34;http://tomcat.apache.org/tomcat-7.0-doc/changelog.html#Tomcat_7.0.73_(violetagg)&#34;&gt;Changelog&lt;/a&gt;得知，tomcat 7.0.73版本添加了&lt;strong&gt;Add additional checks for valid characters to the HTTP request line parsing so invalid request lines are rejected sooner&lt;/strong&gt;导致。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求前自行转义&lt;/li&gt;
&lt;li&gt;更换tomcat为较低版本(不过tomcat的这次更改是依据RFC7230 and RFC 3986,在往后的版本,不会移除该特性)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>不重启解决Too Many Connections</title>
      <link>https://ops.m114.org/post/solve-too-many-connections/</link>
      <pubDate>Sat, 07 Jan 2017 19:57:32 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/solve-too-many-connections/</guid>
      <description>&lt;p&gt;当发生Too many connections时，即使是DBA也无法登录到数据库，一般的做法是修改配置文件的max_connections参数，然后重启数据库，这样业务就有几秒钟的中断，对于线上不能中断的数据库就只能采用另外一种极客的方法了，用gdb直接修改mysqld内存中max_connections的值，具体做法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gdb -p $(cat /data/mysql/mysql-server.pid) -ex &amp;quot;set max_connections=3000&amp;quot; -batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;改进方法如下&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通常有两个参数控制控制最大连接数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_connections：该实例允许最大的连接数
max_user_connections：该实例允许每个用户的最大连接数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个人要根据自己业务量，设置合适的值，不要盲目设置过大，但也不可设置过小，因为MySQL在连接数上升的情况下性能下降非常厉害，如果需要大量连接，这时可以引入thread_pool，所以我们需要保持一个原则：系统创建的用户（给应用使用用户）数 * max_user_connections &amp;lt; max_connections。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在 OpenResty 中使用正则表达式</title>
      <link>https://ops.m114.org/post/use-regular-expressions-in-OpenResty/</link>
      <pubDate>Sun, 30 Oct 2016 10:48:19 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/use-regular-expressions-in-OpenResty/</guid>
      <description>&lt;p&gt;在 OpenResty 中使用正则表达式，社区中推荐的做法是使用ngx.re api。比如匹配一个字符串是否为 http(s) 的链接，可以这么写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local function is_http_url(s)
    return ngx.re.find(s, [[^https?://[\w-_?.:/+=&amp;amp;#%]+$]])
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;压测一下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local t = os.clock()
for _ = 1, max do
    is_http_url(&amp;quot;http://blog.stackoverflow.com/2016/10/Stack-Overflow-92-Podcast-The-Guerilla-Guide-to-Interviewing/?cb=1&amp;quot;)
end
print(&amp;quot;Time cost: &amp;quot;, os.clock() - t, &amp;quot; s&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果：Time cost: 2.663408 s&lt;/p&gt;

&lt;p&gt;另一种做法是使用 lua 的正则语法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local function is_http_url(s)
    return s:find(&amp;quot;^https?://[%w-_%.%?:/%+=&amp;amp;#%%]+$&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果：Time cost: 0.652221 s&lt;/p&gt;

&lt;p&gt;呃，怎么前者耗时是后者的四倍？lua 内置的小小状态机实现，居然打败了大名鼎鼎的 PCRE 库！说好的社区推荐呢！&lt;/p&gt;

&lt;p&gt;仔细一瞧，前者的确漏了点东西。ngx.re默认不会缓存正则表达式编译后的结果。一般在其它编程平台上，我们都会先把字符串编译成正则表达式，再用到正则函数中。比如在
Python 里使用 re.compile。所以赶紧补上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return ngx.re.find(s, [[^https?://[\w-_?.:/+=&amp;amp;#%]+$]], &amp;quot;o&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好，这次性能有了明显提升：Time cost: 0.646518 s&lt;/p&gt;

&lt;p&gt;不错不错，虽然还是跟 lua 的实现不分上下，考虑到 lua 本身的正则支持非常弱（比如连 (foo|bar)
这种形式都不行），而且语法离经叛道，改用 ngx.re 还是挺有必要的。毕竟 PCRE 可是 Perl Compatibility Regex
Expression库，我最喜欢它支持的(?name:pattern)形式的命名捕获功能。&lt;/p&gt;

&lt;p&gt;其实 ngx.re 实现尚未用尽全力呢。开启了 JIT 之后，PCRE 库的性能会更上一层楼：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return ngx.re.find(s, [[^https?://[\w-_?.:/+=&amp;amp;#%]+$]], &amp;quot;jo&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果：Time cost: 0.421824 s&lt;/p&gt;

&lt;p&gt;此时，lua 正则已经被甩到后面去了。&lt;/p&gt;

&lt;p&gt;还能更快吗？&lt;/p&gt;

&lt;p&gt;当然，OpenResty 军火库里还有另外一个武器：&lt;a href=&#34;https://github.com/openresty/lua-resty-core&#34;&gt;lua-resty-core&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require &#39;resty.core.regex&#39;

local function is_http_url(s)
    return ngx.re.find(s, [[^https?://[\w-_?.:/+=&amp;amp;#%]+$]], &amp;quot;jo&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果：Time cost: 0.175346 s&lt;/p&gt;

&lt;p&gt;Boom！最终用时是 lua 正则的四分之一。lua 正则已经望尘莫及了。有趣的是，这个结果正好是第一次比较的结果倒过来。
实话说，这个结果在我的意料之外。resty.core.regex 版本的 ngx.re api，跟默认版本的区别在于对入参和出参的处理是在 lua
代码里完成的，另外调用 C 函数部分采用的是 ffi 而非传统的 C binding。但为什么会这么快？luajit 是否对 ffi 有 jit 优化？&lt;/p&gt;

&lt;p&gt;需要注明一下，resty.core.regex并非银弹。在我们自己的应用上，我尝试引入resty.core.regex，发现对性能无可见的提升。当然，我们的应用的功能不是匹配
url，正则处理亦非瓶颈。不过 resty.core.regex 对自己的项目是否有效，还需要诸君自己测试一番。&lt;/p&gt;

&lt;p&gt;总结&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果在 OpenResty 项目中需要使用正则表达式，请使用 ngx.re api，并开启 jo 选项。&lt;/li&gt;
&lt;li&gt;resty.core.regex 值得一试。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;原文：&lt;a href=&#34;https://segmentfault.com/a/1190000007298100&#34;&gt;https://segmentfault.com/a/1190000007298100&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[python]统计列表中重复项的出现次数</title>
      <link>https://ops.m114.org/post/python-count-duplicate-values-of-list/</link>
      <pubDate>Sun, 30 Oct 2016 10:27:24 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/python-count-duplicate-values-of-list/</guid>
      <description>&lt;p&gt;列表项由数字、字符串组成，统计重复项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from collections import defaultdict
&amp;gt;&amp;gt;&amp;gt; d = defaultdict(int)
&amp;gt;&amp;gt;&amp;gt; for x in [1, 2, 3, 1, 2, 3, 1]:
...     d[x] += 1
...
&amp;gt;&amp;gt;&amp;gt; dict(d)
{1: 3, 2: 2, 3: 2}
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; c = defaultdict(int)
&amp;gt;&amp;gt;&amp;gt; for y in [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;c&#39;]:
...     c[y] += 1
...
&amp;gt;&amp;gt;&amp;gt; dict(c)
{&#39;a&#39;: 2, &#39;c&#39;: 2, &#39;b&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列表项由字典组成，统计某一键值的重复数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; e = defaultdict(int)
&amp;gt;&amp;gt;&amp;gt; for x in [{&#39;a&#39;: 1, &#39;b&#39;: 1}, {&#39;a&#39;: 2, &#39;b&#39;:1}, {&#39;a&#39;: 1, &#39;c&#39;: 3}]:
...     e[x[&#39;a&#39;]] += 1
...
&amp;gt;&amp;gt;&amp;gt; dict(e)   # &#39;a&#39;: 1 出现2次，&#39;a&#39;: 2 出现1次
{1: 2, 2: 1}
&amp;gt;&amp;gt;&amp;gt; e.items()
[(1, 2), (2, 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/14374568/counting-duplicate-words-in-python-the-fastest-way&#34;&gt;dictionary - counting duplicate words in python the fastest way - Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://ops.m114.org/about/</link>
      <pubDate>Sat, 22 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ops.m114.org/about/</guid>
      <description>

&lt;h2 id=&#34;收藏站点&#34;&gt;收藏站点&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://moonbingbing.gitbooks.io/openresty-best-practices/content/index.html&#34;&gt;OpenResty最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://imququ.com&#34;&gt;Jerry Qu HTTPS最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;python常用模块&#34;&gt;Python常用模块&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kennethreitz/requests&#34;&gt;requests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kennethreitz/records&#34;&gt;records&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kennethreitz/tablib&#34;&gt;tablib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/yapf&#34;&gt;yapf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/keyring&#34;&gt;keyring&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;python-web开发&#34;&gt;Python Web开发&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pallets/flask&#34;&gt;Flask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sdispater/flask-orator&#34;&gt;Flask-Orator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/channelcat/sanic&#34;&gt;Sanic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ngx-lua常用模块&#34;&gt;ngx_lua常用模块&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-core&#34;&gt;lua-resty-core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-string&#34;&gt;lua-resty-string&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-mysql&#34;&gt;lua-resty-mysql&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-redis&#34;&gt;lua-resty-redis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudflare/lua-resty-logger-socket&#34;&gt;lua-resty-logger-socket&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pintsized/lua-resty-http&#34;&gt;lua-resty-http&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-upload&#34;&gt;lua-resty-upload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/srcache-nginx-module&#34;&gt;srcache-nginx-module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/echo-nginx-module&#34;&gt;echo-nginx-module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/headers-more-nginx-module&#34;&gt;headers-more-nginx-module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/set-misc-nginx-module&#34;&gt;set-misc-nginx-module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-limit-traffic&#34;&gt;lua-resty-limit-traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-dns&#34;&gt;lua-resty-dns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-lock&#34;&gt;lua-resty-lock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/juce/lua-resty-shell&#34;&gt;lua-resty-shell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bungle/lua-resty-template&#34;&gt;lua-resty-template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-upstream-nginx-module&#34;&gt;lua-upstream-nginx-module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openresty/lua-resty-upstream-healthcheck&#34;&gt;lua-resty-upstream-healthcheck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/upyun/lua-resty-checkups&#34;&gt;lua-resty-checkups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/agentzh/lua-resty-balancer&#34;&gt;lua-resty-balancer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/thibaultcha/lua-resty-jit-uuid&#34;&gt;lua-resty-jit-uuid&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ngx-lua-web开发&#34;&gt;ngx_lua Web开发&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sumory/lor&#34;&gt;Lor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tylerneylon.com/a/learn-lua/&#34;&gt;Learn Lua in 15 Minutes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Ceph核心概念备忘录</title>
      <link>https://ops.m114.org/post/ceph-key-concepts-backup/</link>
      <pubDate>Fri, 26 Aug 2016 14:38:12 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/ceph-key-concepts-backup/</guid>
      <description>

&lt;h2 id=&#34;scrub&#34;&gt;scrub&lt;/h2&gt;

&lt;p&gt;ceph-osd会定义启动scrub线程，扫描部分对象（哪些对象？），和其他副本比较，发现是否一致。如果发现不一致，ceph会抛出这个异常给用户解决。以PG为粒度，触发scrub。用户手动修复，使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph pg repair &amp;lt;pg_id&amp;gt; # 全量复制master节点数据到副本节点。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;scrub分为light scrubbing和Deep scrubbing，前者是频率多直接检查hash值，后者是频率少直接读取内容计算checksum比较。&lt;/p&gt;

&lt;h2 id=&#34;backfill&#34;&gt;backfill&lt;/h2&gt;

&lt;p&gt;当加入或者减少一个新的osd时，所有remapped之后的PG都要迁移到该osd上，此时就叫做backfill。&lt;/p&gt;

&lt;h2 id=&#34;recovery&#34;&gt;recovery&lt;/h2&gt;

&lt;p&gt;当一个osd或者多个osd崩溃之后，再次上线，该osd的状态已经严重滞后了（此时crushmap中还保持该osd）,这个时候就会进行recovery过程。如果是多个osd recovery, 那么这个时候会占用非常多的服务器资源。&lt;/p&gt;

&lt;h2 id=&#34;peering&#34;&gt;peering&lt;/h2&gt;

&lt;p&gt;故障恢复时，对比各个副本的PGlog, 根据PGlog差异构造missing列表，恢复阶段根据missing列表来恢复。peering以PG为单位进行，peering过程中，改PG的IO会被挂起，进入recovery阶段，则可以接受IO，但hit到missing列表项的，也会挂起，直到恢复完成后。因为PGlog的记录是有限的，当peering时发现，PGlog差异太大，则会触发backfill。&lt;/p&gt;

&lt;h2 id=&#34;active-clean&#34;&gt;active + clean&lt;/h2&gt;

&lt;p&gt;PG的status，active的意思是说该PG可以接受读写请求，clean的意思是说PG的副本数达到了要求。&lt;/p&gt;

&lt;h2 id=&#34;degrade&#34;&gt;degrade&lt;/h2&gt;

&lt;p&gt;PG的副本数没有达到要求，但是满足最小副本数要求。&lt;/p&gt;

&lt;h2 id=&#34;incomplete&#34;&gt;incomplete&lt;/h2&gt;

&lt;p&gt;PG的副本数连最小副本数都没有达到。&lt;/p&gt;

&lt;h2 id=&#34;inconsistent&#34;&gt;inconsistent&lt;/h2&gt;

&lt;p&gt;scrub或者deep scrub的时候发现PG内容不一致。&lt;/p&gt;

&lt;h2 id=&#34;down&#34;&gt;down&lt;/h2&gt;

&lt;p&gt;关键数据丢失。进入这个状态的一种方法，比如一个PG有两个副本，先down掉其中的一个osd，再down掉第二个osd，最后把第一个osd起起来，这样这个PG就处于down状态。&lt;/p&gt;

&lt;h2 id=&#34;pglog和日志文件系统&#34;&gt;PGlog和日志文件系统&lt;/h2&gt;

&lt;p&gt;PGlog相当于undo log, journal相当于redo log。一个是在某个操作执行完成之后，做log记录，如果操作成功，则可以undo；另一个是在某个操作执行之前，做log记录，如果操作失败，下次可以redo。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ceph pgs inconsistent</title>
      <link>https://ops.m114.org/post/ceph-pgs-inconsistent/</link>
      <pubDate>Wed, 17 Aug 2016 23:44:59 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/ceph-pgs-inconsistent/</guid>
      <description>&lt;p&gt;OSD扩容后出现如下错误&lt;/p&gt;

&lt;p&gt;&lt;code&gt;HEALTH_ERR 2 pgs inconsistent; 320 scrub errors&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ceph01 ~]# ceph health detail
HEALTH_ERR 2 pgs inconsistent; 320 scrub errors
pg 10.9 is active+clean+inconsistent, acting [6,0,9]
pg 10.10 is active+clean+scrubbing+deep+inconsistent, acting [2,6,4]
320 scrub errors
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修复处于不一致状态的pgs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ceph01 ~]# ceph pg repair 10.9
instructing pg 10.9 on osd.6 to repair

[root@ceph01 ~]# ceph pg repair 10.10
instructing pg 10.10 on osd.2 to repair
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过一段时间的修复后，ceph恢复正常。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>删除osd的正确方式</title>
      <link>https://ops.m114.org/post/right-steps-delete-ceph-osd/</link>
      <pubDate>Sat, 06 Aug 2016 23:51:26 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/right-steps-delete-ceph-osd/</guid>
      <description>

&lt;p&gt;按照官网的步骤走的话，在 &lt;code&gt;标记osd为out&lt;/code&gt; 和 &lt;code&gt;从crushmap删除osd&lt;/code&gt; 这两步都会触发数据再平衡，如下方式只触发了一次迁移，建议使用。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;h2 id=&#34;调整osd的crush-weight&#34;&gt;调整osd的crush weight&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd crush reweight osd.0 0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：这个地方如果想慢慢的调整就分几次将crush 的weight 减低到0 ，这个过程实际上是让数据不分布在这个节点上，让数据慢慢的分布到其他节点上，直到最终为没有分布在这个osd，并且迁移完成这个地方不光调整了osd 的crush weight ，实际上同时调整了host 的 weight ，这样会调整集群的整体的crush 分布，在osd 的crush 为0 后， 再对这个osd的任何删除相关操作都不会影响到集群数据的分布。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;停止osd进程&#34;&gt;停止osd进程&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;systemctl stop ceph-osd@0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是通知集群这个osd进程不在了，不提供服务了，因为本身没权重，就不会影响到整体的分布，也就没有迁移&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;将节点状态标记为out&#34;&gt;将节点状态标记为out&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd out osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是通知集群这个osd不再映射数据了，不提供服务了，因为本身没权重，就不会影响到整体的分布，也就没有迁移&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;从crush中移除节点&#34;&gt;从crush中移除节点&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd crush remove osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是从crush中删除，因为已经是0了 所以没影响主机的权重，也就没有迁移了&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;删除节点&#34;&gt;删除节点&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph osd rm osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是从集群里面删除这个节点的记录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;删除节点认证-不删除编号会占住&#34;&gt;删除节点认证（不删除编号会占住)&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ceph auth del osd.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是从认证当中删除这个节点的信息&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Centos 7.x systemd对比Centos 6.x daemon</title>
      <link>https://ops.m114.org/post/CentOS7-systemd-vs-CentOS6-daemon/</link>
      <pubDate>Sat, 06 Aug 2016 13:54:53 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/CentOS7-systemd-vs-CentOS6-daemon/</guid>
      <description>

&lt;p&gt;从CentOS 7.x开始，CentOS开始使用systemd服务来代替daemon，原来管理系统启动和管理系统服务的相关命令全部由systemctl命令来代替。&lt;/p&gt;

&lt;h1 id=&#34;1-原来的-service-命令与-systemctl-命令对比&#34;&gt;1、原来的 service 命令与 systemctl 命令对比&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;daemon命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;systemctl命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;service [服务] start&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl start [unit type]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;启动服务&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;service [服务] stop&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl stop [unit type]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;停止服务&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;service [服务] restart&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl restart [unit type]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;重启服务&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;此外还有二个systemctl参数没有与service命令参数对应&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;status: 参数来查看服务运行情况
reload: 重新加载服务，加载更新后的配置文件（并不是所有服务都支持这个参数，比如network.service）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;应用举例:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#启动网络服务
systemctl start network.service
#停止网络服务
systemctl stop network.service
#重启网络服务
systemctl restart network.service
#查看网络服务状态
systemctl status network.serivce
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;2-原来的chkconfig-命令与-systemctl-命令对比&#34;&gt;2、原来的chkconfig 命令与 systemctl 命令对比&lt;/h1&gt;

&lt;h2 id=&#34;2-1-设置开机启动-不启动&#34;&gt;2.1、设置开机启动/不启动&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;daemon命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;systemctl命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;chkconfig [服务] on&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl enable [unit type]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;设置服务开机启动&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;chkconfig [服务] off&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl disable [unit type]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;设备服务禁止开机启动&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;应用举例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#停止cup电源管理服务
systemctl stop cups.service
#禁止cups服务开机启动
systemctl disable cups.service
#查看cups服务状态
systemctl status cups.service
#重新设置cups服务开机启动
systemctl enable cups.service
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-2-查看系统上所有的服务&#34;&gt;2.2、查看系统上所有的服务&lt;/h2&gt;

&lt;p&gt;命令格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl [command] [--type=TYPE] [--all]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数详解：&lt;/p&gt;

&lt;p&gt;command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;list-units: 依据unit列出所有启动的unit。加上 --all 才会列出没启动的unit;
list-unit-files: 依据 /usr/lib/systemd/system/ 内的启动文件，列出启动文件列表
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;--type=TYPE&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;为unit type, 主要有service, socket, target, timer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;应用举例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl                  #列出所有的系统服务
systemctl list-units       #列出所有启动unit
systemctl list-unit-files  #列出所有启动文件
systemctl list-units --type=service --all  #列出所有service类型的unit
systemctl list-units --type=socket --all   #列出所有socket类型的unit
systemctl list-units --type=timer --all    #列出所有timer类型的unit
systemctl list-units --type=target --all   #列出所有target
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-systemctl特殊的用法&#34;&gt;3、systemctl特殊的用法&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;#查看网络服务是否启动
systemctl is-active network.service
#检查网络服务是否设置为开机启动
systemctl is-enable network.service
#停止cups服务
systemctl stop cups.service
#注销cups服务
systemctl mask cups.service
#查看cups服务状态
systemctl status cups.service
#取消注销cups服务
systemctl unmask cups.service
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-init-命令与systemctl命令对比&#34;&gt;4、init 命令与systemctl命令对比&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;init命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;systemctl 命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;init 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl poweroff&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;系统关机&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;init 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;systemctl reboot&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;重新启动&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;与开关机相关的其他命令：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;systemctl命令&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;systemctl suspend&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;进入睡眠模式&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;systemctl hibernate&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;进入休眠模式&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;systemctl rescue&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;强制进入救援模式&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;systemctl emergency&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;强制进入紧急救援模式&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;5-设置系统运行级别&#34;&gt;5、设置系统运行级别&lt;/h1&gt;

&lt;h2 id=&#34;5-1-运行级别对应表&#34;&gt;5.1、运行级别对应表&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;init级别&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;systemctl target&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;shutdown.target&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;emergency.target&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;rescure.target&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;multi-user.target&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;无&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;graphical.target&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;无&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;此外还有一个getty.target用来设置tty的数量。&lt;/p&gt;

&lt;h2 id=&#34;5-2-设置运行级别&#34;&gt;5.2、设置运行级别&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;systemctl get-default  #获得当前的运行级别
systemctl set-default multi-user.target  #设置默认的运行级别为mulit-user
systemctl isolate multi-user.target      #在不重启的情况下，切换到运行级别mulit-user下
systemctl isolate graphical.target       #在不重启的情况下，切换到图形界面下
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;6-关闭网络服务&#34;&gt;6、关闭网络服务&lt;/h1&gt;

&lt;p&gt;在使用systemctl关闭网络服务时有一些特殊，需要同时关闭unit.servce和unit.socket&lt;/p&gt;

&lt;p&gt;使用systemctl查看开启的sshd服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost system]# systemctl list-units --all | grep sshd
sshd-keygen.service loaded inactive dead        OpenSSH Server Key Generation
sshd.service        loaded active   running     OpenSSH server daemon
sshd.socket         loaded inactive dead        OpenSSH Server Socket
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到系统同时开启了sshd.service和sshd.socket , 如果只闭关了sshd.service那么sshd.socket还在监听网络，在网络上有要求连接sshd时就会启动sshd.service。因此如果要完全关闭sshd服务，需要同时停用sshd.service和sshd.socket。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl stop sshd.service
systemctl stop sshd.socket
systemctl disable sshd.service sshd.socket
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>MySQL Unable to lock ibdata1 error 11 fix</title>
      <link>https://ops.m114.org/post/MySQL-Unable-to-lock-ibdata1-error-11-fix/</link>
      <pubDate>Wed, 03 Aug 2016 10:52:24 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/MySQL-Unable-to-lock-ibdata1-error-11-fix/</guid>
      <description>

&lt;p&gt;A bad shutdown can cause such erros on MySQL.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InnoDB: Unable to lock ./ibdata1, error: 11
InnoDB: Check that you do not already have another mysqld process
InnoDB: using the same InnoDB data or log files.
InnoDB: Error in opening ./ibdata1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;for-solution&#34;&gt;For solution&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;mv ibdata1 ibdata1.bak
cp -a ibdata1.bak ibdata1
service mysqld restart
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ceph集群jewel版本部署osd激活权限报错</title>
      <link>https://ops.m114.org/post/ceph-jewel-osd-activate-bug/</link>
      <pubDate>Fri, 29 Jul 2016 00:29:29 +0800</pubDate>
      
      <guid>https://ops.m114.org/post/ceph-jewel-osd-activate-bug/</guid>
      <description>&lt;p&gt;&lt;strong&gt;环境&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)&lt;/br&gt;
ceph-deploy 1.5.34&lt;/p&gt;

&lt;p&gt;ceph集群jewel版本部署过程中执行osd激活操作如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate ceph13:/dev/sdb1:/dev/sda2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错内容如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2016-07-29 00:05:19,106][ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[2016-07-29 00:05:19,107][ceph_deploy.cli][INFO  ] Invoked (1.5.34): /bin/ceph-deploy osd activate ceph13:/dev/sdb1:/dev/sda2
[2016-07-29 00:05:19,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  username                      : None
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  verbose                       : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  quiet                         : False
[2016-07-29 00:05:19,108][ceph_deploy.cli][INFO  ]  cd_conf                       : &amp;lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x26dcb90&amp;gt;
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  func                          : &amp;lt;function osd at 0x26d0320&amp;gt;
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  default_release               : False
[2016-07-29 00:05:19,109][ceph_deploy.cli][INFO  ]  disk                          : [(&#39;ceph13&#39;, &#39;/dev/sdb1&#39;, &#39;/dev/sda2&#39;)]
[2016-07-29 00:05:19,110][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks ceph13:/dev/sdb1:/dev/sda2
[2016-07-29 00:05:19,217][ceph13][DEBUG ] connected to host: ceph13
[2016-07-29 00:05:19,218][ceph13][DEBUG ] detect platform information from remote host
[2016-07-29 00:05:19,258][ceph13][DEBUG ] detect machine type
[2016-07-29 00:05:19,264][ceph13][DEBUG ] find the location of an executable
[2016-07-29 00:05:19,266][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.2.1511 Core
[2016-07-29 00:05:19,266][ceph_deploy.osd][DEBUG ] activating host ceph13 disk /dev/sdb1
[2016-07-29 00:05:19,266][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2016-07-29 00:05:19,266][ceph13][DEBUG ] find the location of an executable
[2016-07-29 00:05:19,270][ceph13][INFO  ] Running command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2016-07-29 00:05:19,598][ceph13][WARNING] main_activate: path = /dev/sdb1
[2016-07-29 00:05:19,598][ceph13][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2016-07-29 00:05:19,598][ceph13][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2016-07-29 00:05:19,630][ceph13][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2016-07-29 00:05:19,638][ceph13][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2016-07-29 00:05:19,803][ceph13][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2016-07-29 00:05:19,967][ceph13][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.K5WBsO with options noatime,inode64
[2016-07-29 00:05:19,967][ceph13][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:19,983][ceph13][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:19,991][ceph13][WARNING] activate: Cluster uuid is f2694afb-b5fc-4225-982a-342c4a1ca389
[2016-07-29 00:05:19,991][ceph13][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: Cluster name is ceph
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: OSD uuid is 684effdc-67ae-4f54-a9b8-a113f4a8f0cc
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: OSD id is 0
[2016-07-29 00:05:20,156][ceph13][WARNING] activate: Initializing OSD...
[2016-07-29 00:05:20,157][ceph13][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap
[2016-07-29 00:05:20,572][ceph13][WARNING] got monmap epoch 1
[2016-07-29 00:05:20,572][ceph13][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap --osd-data /var/lib/ceph/tmp/mnt.K5WBsO --osd-journal /var/lib/ceph/tmp/mnt.K5WBsO/journal --osd-uuid 684effdc-67ae-4f54-a9b8-a113f4a8f0cc --keyring /var/lib/ceph/tmp/mnt.K5WBsO/keyring --setuser ceph --setgroup ceph
[2016-07-29 00:05:20,686][ceph13][WARNING] mount_activate: Failed to activate
[2016-07-29 00:05:20,686][ceph13][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:20,687][ceph13][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.K5WBsO
[2016-07-29 00:05:21,052][ceph13][WARNING] Traceback (most recent call last):
[2016-07-29 00:05:21,052][ceph13][WARNING]   File &amp;quot;/usr/sbin/ceph-disk&amp;quot;, line 9, in &amp;lt;module&amp;gt;
[2016-07-29 00:05:21,052][ceph13][WARNING]     load_entry_point(&#39;ceph-disk==1.0.0&#39;, &#39;console_scripts&#39;, &#39;ceph-disk&#39;)()
[2016-07-29 00:05:21,052][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 4994, in run
[2016-07-29 00:05:21,053][ceph13][WARNING]     main(sys.argv[1:])
[2016-07-29 00:05:21,053][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 4945, in main
[2016-07-29 00:05:21,053][ceph13][WARNING]     args.func(args)
[2016-07-29 00:05:21,053][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3299, in main_activate
[2016-07-29 00:05:21,053][ceph13][WARNING]     reactivate=args.reactivate,
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3056, in mount_activate
[2016-07-29 00:05:21,054][ceph13][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 3232, in activate
[2016-07-29 00:05:21,054][ceph13][WARNING]     keyring=keyring,
[2016-07-29 00:05:21,054][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 2725, in mkfs
[2016-07-29 00:05:21,055][ceph13][WARNING]     &#39;--setgroup&#39;, get_ceph_group(),
[2016-07-29 00:05:21,055][ceph13][WARNING]   File &amp;quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&amp;quot;, line 2672, in ceph_osd_mkfs
[2016-07-29 00:05:21,055][ceph13][WARNING]     raise Error(&#39;%s failed : %s&#39; % (str(arguments), error))
[2016-07-29 00:05:21,055][ceph13][WARNING] ceph_disk.main.Error: Error: [&#39;ceph-osd&#39;, &#39;--cluster&#39;, &#39;ceph&#39;, &#39;--mkfs&#39;, &#39;--mkkey&#39;, &#39;-i&#39;, &#39;0&#39;, &#39;--monmap&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/activate.monmap&#39;, &#39;--osd-data&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO&#39;, &#39;--osd-journal&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/journal&#39;, &#39;--osd-uuid&#39;, &#39;684effdc-67ae-4f54-a9b8-a113f4a8f0cc&#39;, &#39;--keyring&#39;, &#39;/var/lib/ceph/tmp/mnt.K5WBsO/keyring&#39;, &#39;--setuser&#39;, &#39;ceph&#39;, &#39;--setgroup&#39;, &#39;ceph&#39;] failed : 2016-07-29 00:05:20.672440 7f46517dd800 -1 filestore(/var/lib/ceph/tmp/mnt.K5WBsO) mkjournal error creating journal on /var/lib/ceph/tmp/mnt.K5WBsO/journal: (13) Permission denied
[2016-07-29 00:05:21,056][ceph13][WARNING] 2016-07-29 00:05:20.672462 7f46517dd800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -13
[2016-07-29 00:05:21,056][ceph13][WARNING] 2016-07-29 00:05:20.672526 7f46517dd800 -1  ** ERROR: error creating empty object store in /var/lib/ceph/tmp/mnt.K5WBsO: (13) Permission denied
[2016-07-29 00:05:21,056][ceph13][WARNING]
[2016-07-29 00:05:21,057][ceph13][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2016-07-29 00:05:21,057][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;解决办法:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将ceph集群需要使用的所有磁盘权限，所属用户、用户组改给ceph&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chown ceph:ceph /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;问题延伸:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;此问题本次修复后，系统重启磁盘权限会被修改回，导致osd服务无法正常启动，这个权限问题很坑，每次系统启动自动修改磁盘权限。&lt;/p&gt;

&lt;p&gt;查找ceph资料，发现这其实是一个bug，社区暂未解决。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tracker.ceph.com/issues/13833&#34;&gt;http://tracker.ceph.com/issues/13833&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>